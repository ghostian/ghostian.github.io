---
layout: post
title:  'Machine Learning Recaps'
categories: Machine-Learning
date: 2019-09-10
---
- [Machine Learning (10601)](#machine-learning-10601)
  - [1. Decision Tree (Project)](#1-decision-tree-project)
  - [2. KNN](#2-knn)
  - [3. Perceptron](#3-perceptron)
  - [4. Linear Regression](#4-linear-regression)
  - [5. Bayes Optimal Classifier](#5-bayes-optimal-classifier)
  - [6. Logsitic Regression (Project)](#6-logsitic-regression-project)
  - [7. Neural Network (Project)](#7-neural-network-project)
  - [8. PAC](#8-pac)
  - [9. RNN](#9-rnn)
  - [10. MLE/MAP](#10-mlemap)
  - [11. HMM (Project)](#11-hmm-project)
  - [12. Bayesian Networks](#12-bayesian-networks)
  - [13. Reinforement learning (Project)](#13-reinforement-learning-project)
  - [14. K Means](#14-k-means)
  - [15. PAC & AdaBoost](#15-pac--adaboost)
  - [16. SVM](#16-svm)
  - [17. Ensemble + recommender systems](#17-ensemble--recommender-systems)
  - [N-gram (Project)](#n-gram-project)


---

### Machine Learning (10601)

#### 1. Decision Tree (Project)

Predicted political parties by implementing Decision Tree with ID3 algorithm, an 85% accuracy was achieved on test set.

- Bernoulli Dsitribution
- Find Best Attribute according to
  1. Error Rate (Lowest)
  2. Gini Impurity (Lowest)
  3. **Mutual Information (Highest)**
- ID3/CART Algorithm
  - ID3: Choose the biggest Mutual Information as splitting protocal
  - C4.5: Improvement of ID3, overcomed the issue of cannont handle the continuous property
- Information Theory
  - Specific Conditional Entropy
  - Conditional Entropy
  - Mutual Information

#### 2. KNN

#### 3. Perceptron

- Online Learning
- $h(x)=Sign(\theta^T x)=Sign(\vec w^T \vec x^i +b)$
  - Tune parameter according to: Pos/Neg mistake
- Def of Hyperplane in different dimensions
- Validation set: split training data into train-sub and validation
- Online vs. Batch learning
- Inductive Bias
  - Deicison boundary is linear
  - Prefer to correct most recent mistakes
- Does Suffer `Overfitting`

#### 4. Linear Regression

- Gradient descent
  - $\nabla J \triangleq= [{\delta J(\theta))\over(\delta \theta_1)},{\delta J(\theta))\over(\delta \theta_2)},...,{\delta J(\theta))\over(\delta \theta_m)}]^T$
  - pros: Simple & often effective | Often very scalable
  - cons: Only applies to smooth functions (differentiable) | Might find a local minimum
- Closed Form Optimization
- Zero Derivative: Maximum / Minimum / Saddle Point
- Convexity
  - Convex: local minimum is global minimum
  - Nonconvex: Local minimum is not necessarily a global minimum
  - Strictly convex function ha s uniqe global minimum
- Oridinary Least Squares problem
- Epoch : single pass through the training data
  - GD, `one` update per epoch
  - SGD, `N` updates per epoch (N = # train examples)
  - SGD reduces MSE much more rapidly than GD
  - For GD/SGD, training MSE is initially large due to uniformed initilization

#### 5. Bayes Optimal Classifier

- Reduceable Error/ Ireduceable Error
- Maximum Likelihood Estimation (MLE)

#### 6. Logsitic Regression (Project)

Built a sentiment polarity analyzer, which will be capable of analyzing the overall sentiment polarity (positive or negative) using `Logistic Regression`, 98.6% of training accuracy and 90.25% testing accuracy was achieved according to the given dataset. (Used SGD)

- Pros
  1. While Z approches $\infin$, function result approches to 1, while Z approches $-\infin$, function result approches to 0, this is good for probablistic models
  2. Easy to calculate derivative
  3. Fast to train
- Sigmoid function $1 \over {1+e^{-z}}$
  - Y is discrete and uncontinuous, use sigmoid to make this function can be only classified as A or B in certian area
- L1 Regularization
  - Create more sparsity
  - Force $\omega$ equal to 0
  - Descent speed: Absolute function
- L2 Regularization
  - Force $\omega$ close to 0 but not equal to 0
  - Descen spped: Quadratic function
- Multinomial LR

#### 7. Neural Network (Project)

Built a handwriting recognition system using a neural network. Achieved 100% accuracy with 20 hidden units and 100 epochs on OCR hand-written letters dataset.

- Hidden layers
- Hidden units (neuron)
- Active Functions
- Deep layer NN are actually sasier for training. (Empirically tested)
- Back Propagation
- Mini batch SGD: Approximate true gradient by the average gradient of K randomly chosen examples

#### 8. PAC 

#### 9. RNN

- By unrolling RNN through time, we can **share parameters** and accmmodate arbitrary length input/output pairs
- Applications: **Time-serires data** such as sentences, speech, stock-market, signal data, etc.

#### 10. MLE/MAP

- MLE: Maximum Likelihood Estimation
- MAP: Maximum a posteriori Estimation
- Shattering
- VC-Dimension
- Sample Complexity: Realizable | Agnostic
- Closed-form MLE

#### 11. HMM (Project)

implement a new named entity recognition system using Hidden Markov Models with Forward Backward Algorithm, achived negative log likelihood of -0.924531673593 and 100% accuracy on test set.

- Viterbi / Forward-BackWard
- Shortcomings
  1. Capture dependencies between each state and only its corresponding observation
  2. Mismatch between leraning objective function and prediction objective function
- Minmum Bayes Risk Decoding

#### 12. Bayesian Networks

- Joint distribution
- Directed graphical model
- Markov Blanket

#### 13. Reinforement learning (Project)

Implemented Q-learning with function approximation to solve the mountain car environment.

- Q-Learning
- Synchronous | Asynchronous Value Iteration
- Value Iteration Convergence
- Policy Iteration

#### 14. K Means

#### 15. PAC & AdaBoost

#### 16. SVM

- Key Idea: Find a linear seperator with maximum margin
- Linear Seperability
- SVM Dual
- Kernal Trick: Preprocess the data to produce nonlinear features if we want non-linear seperable data want to be linear classified

#### 17. Ensemble + recommender systems

#### N-gram (Project)

Implemented a model interpolates the N-gram probabilities, which predictes the ”special language” used by the lawyers when drafting these privacy policies and compute the output's perplexity.
